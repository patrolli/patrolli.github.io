{"meta":{"title":"patrolli的博客","subtitle":"你要守住内心的火焰","description":"UESTC, Computer Vision, Data Intelligence Group","author":"patrolli","url":"http://yoursite.com","root":"/"},"pages":[{"title":"archives","date":"2019-12-11T09:15:22.000Z","updated":"2019-12-11T10:05:57.213Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-12-11T10:30:35.000Z","updated":"2019-12-11T10:31:02.095Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-12-11T09:31:59.000Z","updated":"2019-12-11T10:42:10.111Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"PyTorch 之torch.cat()和torch.stack()的理解","slug":"19-12-13-02","date":"2019-12-13T14:24:00.000Z","updated":"2019-12-13T18:36:57.445Z","comments":true,"path":"2019/12/13/19-12-13-02/","link":"","permalink":"http://yoursite.com/2019/12/13/19-12-13-02/","excerpt":"看了下官方手册和网上的一些解释，但还是没太弄明白，于是自己下来在草稿纸上画了一下，然后用程序验证，总算把这两个函数搞懂了些。","text":"看了下官方手册和网上的一些解释，但还是没太弄明白，于是自己下来在草稿纸上画了一下，然后用程序验证，总算把这两个函数搞懂了些。 首先给出两个tensor: a和b，它们的形状都是（1，3，2） 判断tensor维度的方法 看对应的中括号，有几个左中括号就有几个维度，如： 有三个左括号，那么a就有3个维度 判断每个维度有多少个元素 对于dim=0，我们找到第一个左中括号，并且找到与之匹配的右中括号，看他们之中有多少个元素（凡是被中括号包裹的视作1个元素，其实就是看配对的中括号中有几个逗号，其中的元素个数就是逗号个数加1） tensor.cat() 将两个（或更多）相同形状的tensor按某一维度(dim)进行拼接，操作是：对两个tensor，将需要拼接的维度中的所有元素接在一起，具体示例如下： tensor.cat((a,b), dim=0) 找到tensor a和tensor b的第0维中所有的元素，将他们拼在一起： tensor.cat((a,b), dim=1) 同样，找到两个tensor第一维中的所有元素，然后拼在一起： tensor.cat((a,b), dim=2) 通过以上步骤，我们不难得出结论，对于cat操作，输出的tensor的形状，就是输入的tensor形状在指定维度(dim)上相加即可，输出的tensor总的维度数不会改变，改变的只是相应维度上元素的个数而已。 tensor.stack() 已经说过对于cat()操作，输出的tensor总的维度数不会改变，而stack()操作则会改变输出tensor总的维度数。假设输入两个tensor，分别是a，b，他们的形状都是(3, 2)，在stack时，我们会先在指定的dim维度上增加一维，然后在增加的这个维度上对这两个tensor做cat操作，具体如下： tensor.stack((a,b), dim=0) tensor.stack((a,b), dim=1) tensor.stack((a,b), dim=2) 所以，对于stack操作，如果我们想知道输出tensor的维度，我们只需要将输入tensor添加一维，然后按照cat()操作来进行就可以了，至于在哪个维度上添加，就取决于参数dim的值。","categories":[],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://yoursite.com/tags/PyTorch/"}]},{"title":"PyTorch MathOperation API总结","slug":"19-12-13-01","date":"2019-12-13T08:04:00.000Z","updated":"2019-12-13T09:32:38.082Z","comments":true,"path":"2019/12/13/19-12-13-01/","link":"","permalink":"http://yoursite.com/2019/12/13/19-12-13-01/","excerpt":"Pytorch Math operations Pointwise Ops 基本运算 torch.add() torch.add(input, other, out=None)--执行加法，out=input+other torch.add(input, other, alpha=1, out=None)--out=input + alpha x other torch.abs(input, out=None)--求绝对值，返回Tensor torch.div(input, other, out=None)--除法运算，这里的other可以是一个标量或者是一个张量 torch.fmod(input, other, out=None)--input中的每个数据对other取余，余数和input中的被除数符号相同 torch.mul(input, other, out=None)--相乘， \\(out_i = input_i \\times other\\)或者\\(out_i = input_i \\times other_i\\) torch.neg(input, out=None)--对每个元素乘以-1 torch.remainder(input, other, out=None)--input中的每个数据对other取余，余数和除数的符号相同","text":"Pytorch Math operations Pointwise Ops 基本运算 torch.add() torch.add(input, other, out=None)--执行加法，out=input+other torch.add(input, other, alpha=1, out=None)--out=input + alpha x other torch.abs(input, out=None)--求绝对值，返回Tensor torch.div(input, other, out=None)--除法运算，这里的other可以是一个标量或者是一个张量 torch.fmod(input, other, out=None)--input中的每个数据对other取余，余数和input中的被除数符号相同 torch.mul(input, other, out=None)--相乘， \\(out_i = input_i \\times other\\)或者\\(out_i = input_i \\times other_i\\) torch.neg(input, out=None)--对每个元素乘以-1 torch.remainder(input, other, out=None)--input中的每个数据对other取余，余数和除数的符号相同 线性运算 torch.addcdiv(input, value=1, tensor1, tensor2, out=None)-- \\(out_i=input_i+value\\times \\frac{tensor1_i}{tensor2_i}\\) torch.addcmul(input, value=1, tensor1, tensor2, out=None)-- \\(out_i=input_i+value \\times tensor1_i \\times tensor2_i\\) torch.lerp(input, end, weight, out=None)--对两个tensor做线性插值，计算 \\(out_i = start_i + weight_i \\times (end_i - start_i)\\) 三角函数 torch.acos(input, out=None)--\\(out_i=cos^{-1}(input_i)\\) torch.asin(input, out)--执行arcsin运算 torch.atan(input, out)--执行arctan运算 torch.atan2(input, other, out=None)--对\\(input_i / other_i\\)做arctan运算 torch.cos(input, out=None)--计算cos torch.cosh(input, out=None)--计算cosh torch.sin(input, out=None)--sin函数 torch.sinh(input, out=None)--sinh函数 torch.tan(input, out=None)--tan函数 torch.tanh(input, out=None)--tanh函数 幂指对函数 torch.exp(input, out=None)--计算\\(y_i = e^{x_i}\\) torch.expm1(input, out=None)--计算\\(y_i = e^{x_i}-1\\) torch.log(input, out=None)--对输入数据计算自然对数 torch.log10(input, out=None)--计算10为底的对数 torch.log1p(input, out=None)--计算\\(y_i = log_e(x_i+1)\\) torch.log2(input, out=None)--计算2为底的对数 torch.pow() torch.pow(input, exponent, out=None)--执行\\(out_i = x_i^{exponent_i}\\) torch.pow(self, exponent, out=None)--执行\\(out_i = self^{exponent_i}\\),这里self是一个标量，而exponent是一个tensor 逻辑运算 torch.bitwise_not(input, out=None)--对输入的数按位取反，这里的input tensor必须是整型或者布尔型数据 torch.logical_not(input, out=None)--对input每个元素做逻辑非运算 torch.logical_xor(input, other, out=None)--对两个tensor的数据做异或运算，这两个tensor必须是布尔类型 取整操作 torch.ceil(input, out=None)--对输入数据向上取整 torch.floor(input, out=None)--对输入数据向下取整 torch.round(input, out=None)--对输入数据四舍五入取整 torch.trunc(input, out=None)--去掉每个元素的小数部分 torch.frac(input, out=None)--得到输入数据的小数部分 torch.clamp(input, min, max, out=None)--将input中的所有元素都截断在[min, max]的范围内，可以省略其中一个参数(min或者max)，表示[-Inf, max]或者[min, Inf] 其它一些常见函数 torch.reciprocal(input, out=None)--执行\\(out_i = \\frac{1}{input_i}\\) torch.rsqrt(input, out=None)--计算\\(out_i=\\frac{1}{\\sqrt{input_i}}\\) torch.sigmoid(input, out=None)--计算\\(out_i = \\frac{1}{1+e^{-input_i}}\\) torch.sign(input, out=None)--符号函数 torch.sqrt(input, out=None)--对输入的每个元素求根号 一些复杂函数 torch.digamma(input, out=None)--计算digmma函数 torch.erf(input, out=None)--计算误差函数 torch.erfc(input, out=None)--计算误差函数的补 torch.erfinv(input, out=None) torch.lgamma(input, out=None) torch.mvlgamma(input, p) torch.polygamma(n, input, out=None)","categories":[],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://yoursite.com/tags/PyTorch/"}]},{"title":"Hexo+GitHub个人博客搭建","slug":"19-12-11-01","date":"2019-12-11T11:49:00.000Z","updated":"2019-12-13T09:26:28.449Z","comments":true,"path":"2019/12/11/19-12-11-01/","link":"","permalink":"http://yoursite.com/2019/12/11/19-12-11-01/","excerpt":"突然心血来潮想搭建一个自己的博客，此前我有在博客园写一些博客的习惯，但看到那些简洁漂亮的个人博客，我也有些心痒想自己搭建一个。今天刚好在B站看到UP主CodeSheep发的一个个人博客搭建指南的视频，于是跟着视频自己也做了一个，虽然视频只有20分钟，但却花了我整整一个下午才勉强算是把博客搭建起来了，我在此记录一下这个过程，以及在中途遇到问题后在网上找到的相应的解决方法，希望对之后想搭建个人博客的小伙伴有所帮助。（以下内容仅针对windows操作系统）","text":"突然心血来潮想搭建一个自己的博客，此前我有在博客园写一些博客的习惯，但看到那些简洁漂亮的个人博客，我也有些心痒想自己搭建一个。今天刚好在B站看到UP主CodeSheep发的一个个人博客搭建指南的视频，于是跟着视频自己也做了一个，虽然视频只有20分钟，但却花了我整整一个下午才勉强算是把博客搭建起来了，我在此记录一下这个过程，以及在中途遇到问题后在网上找到的相应的解决方法，希望对之后想搭建个人博客的小伙伴有所帮助。（以下内容仅针对windows操作系统） 环境准备 需要安装Git、Node.js，通过在命令行输入git --version和 node -v查看是否安装有Git和Node.js以及获得安装的版本号。 同时，为了将博客部署在github上，还需要注册github账号，并将gitbhub账号与本地电脑关联起来。这部分可以参看：廖雪峰-Git教程 博客搭建 使用Hexo框架来搭建我们的个人博客，跟着这个28分钟的视频一步一步来，就可以得到一个最初版本的个人博客啦 手把手教你从0开始搭建自己的个人博客 |无坑版视频教程| hexo 这里总结几个常用的hexo指令： hexo init-- 在当前所在目录新建一个网站 hexo new [filename]--创建一个名为filename的md文件，它被放在source/_post文件夹之下，这里的名字可以随便取，等到我们用md编辑器去写内容时，可以在编辑器内修改标题。 hexo s--在本地端口预览网站 hexo clean--清除缓存文件和已生成的静态文件，这个指令和hexo g配合使用，当我们对hexo的配置进行修改或添加新的文章到博客上时，都可以先hexo clean再hexo g，生成新的静态页面。 hexo g--生成静态文件 hexo d -- 部署网站，将本地的修改推送到远程 更换主题/配置Hexo 其实按着上面的视频一步步做基本没有什么问题，很快就能够完成，真正耗费我时间的是在显示图片、显示公式等这些上面视频没有提到的地方，这里我把我遇到的问题和在网上找到的解决方法都贴在这里。 显示图片 Markdown显示图片总是一个麻烦的事情，要么将图片保存在本地然后在文章中引用图片的相对路径，要么使用第三方的图床生成外部链接。解决Hexo中显示图片的一个有效方法：hexo+github上传图片到博客 显示公式 通常第一步是修改主题(themes)文件夹中的_config.yml，将里面的mathjax参数设置为true(为了加快渲染速度通常主题是会将这个选项关闭的)。 对于yilia主题，上述修改后只能显示一些简单的公式，复杂的公式还是无法显示出来，这个问题可以参见Hexo构建blog时渲染LaTeX数学公式的问题，通过更换Markdown引擎来解决 对于next主题，打开Mathjax开关后，需要在文章的Front-matter中也打开mathjax开关，参见：Hexo 的 Next 主题中渲染 MathJax 数学公式 (加上mathjax: true) yilia主题 在视频中up主演示了将我们博客换成yilia主题的过程，我一开始也是在研究这个主题，但出现一个问题，即是它的tag页面和categories页面在配置后都不能显示内容（一片空白），试了网上的办法都不管用，我也不知道问题出在哪儿，文章分类这个功能对我来说不可或缺，所以在折腾几个小时后放弃了这个主题，但在这个过程中还是找到一些比较有价值的内容。 Hexo yilia 主题一揽子使用方案 yilia主题在查看所有文章时会出现缺失模块的错误，这里照着它的提示来就OK，包括在执行命令cnpm i hexo-generator-json-content --save时，我遇到了报错： 1234567891011F:\\myblog&gt;npm i hexo-generator-json-content --savenpm WARN rm not removing F:\\myblog\\node_modules\\.bin\\swig.cmd as it wasn&#39;t installed by F:\\myblog\\node_modules\\_swig-templates@2.0.3@swig-templatesnpm WARN rm not removing F:\\myblog\\node_modules\\.bin\\swig as it wasn&#39;t installed by F:\\myblog\\node_modules\\_swig-templates@2.0.3@swig-templatesnpm ERR! path F:\\myblog\\node_modules\\hexo-deployer-git\\node_modules\\.bin\\swig.cmdnpm ERR! code EEXISTnpm ERR! Refusing to delete F:\\myblog\\node_modules\\hexo-deployer-git\\node_modules\\.bin\\swig.cmd: is outside F:\\myblog\\node_modules\\hexo-deployer-git\\node_modules\\swig-templates and not a linknpm ERR! File exists: F:\\myblog\\node_modules\\hexo-deployer-git\\node_modules\\.bin\\swig.cmdnpm ERR! Move it away, and try again.npm ERR! A complete log of this run can be found in:npm ERR! C:\\Users\\xxx\\AppData\\Roaming\\npm-cache\\_logs\\2019-12-11T06_25_52_957Z-debug.log 这里就按照报错提示把相应文件移除就OK了。 解决 Hexo 搭建博客显示不出分类、标签问题 这个就是我遇到最头疼的问题了，在yilia主题下没有实现出来，只好作罢，这个解决方法可以参考。 Hexo+Github实现相册功能 这个还没有尝试，这种可有可无的等到之后有闲的时候再弄吧 next主题 在yilia主题折腾了许久还是没弄出来分类、标签的设置，只好换到next主题，结果很快就配置好了=_= ，而且页面看起来也更加舒服，像文章目录啥的都比yilia来得好看。记录一些next主题的操作 显示公式：这个在前面已经提到 一些个性化设置：hexo的next主题个性化配置教程 、 Hexo Next主题启用及相关设置留着之后看的。。 hexo换next主题当出现首页和标签的时候，点进去为cannot GET /20%/,如何解决? 遇到的一个问题 Hexo+NexT（零）：最全Hexo+Next搭建博客教程 没有看的教程。。 最后 七零八落地写了一些，毕竟今天花了可以说是一整天的时间在搞这个了，还是很有必要把这些血泪史记在这里，之后博客的内容也会越来越丰富起来。反思起来，我花了很多时间在这些所谓的工具选择上，比如怎么用什么样的编辑器、是用做电子笔记还是就在纸上写笔记等，但无论哪种方式，它们都只是一种承载知识的载体而已，真正有价值的还是学习过程中我们学到的、理解到的知识，花一些时间和精力在学习方法、学习工具上是可以的，但一定不能本末倒置了，算是对自己的一些警勉吧。","categories":[],"tags":[{"name":"环境配置","slug":"环境配置","permalink":"http://yoursite.com/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"}]},{"title":"LaSO--Label-Set Operations networks for multi-lable few-shot learning","slug":"article-2","date":"2019-12-11T08:39:52.000Z","updated":"2019-12-11T11:13:32.578Z","comments":true,"path":"2019/12/11/article-2/","link":"","permalink":"http://yoursite.com/2019/12/11/article-2/","excerpt":"0.LaSO: Label-Set Operations networks for multi-lable few-shot learning 1. Summary This paper focus on the multi-label few-shot learning, where has no prior work of this task. They proposed Label-Set Operations networks, which can generate new multi-label samples in the feature space, this method can be seen as data augmentation techinques.","text":"0.LaSO: Label-Set Operations networks for multi-lable few-shot learning 1. Summary This paper focus on the multi-label few-shot learning, where has no prior work of this task. They proposed Label-Set Operations networks, which can generate new multi-label samples in the feature space, this method can be seen as data augmentation techinques. 2. Research Objective Find a method to solve the multi-label few-shot learning task and demonstrate the its utility for this work. 3. Problem Statement It'a combination of multi-lable classfication task and few-shot learning task. It would train the network on the multi-label training set and validate or test it on the dataset which has the catagories unseen during training. Need to use the mAP to evaluate the performance. 4. Methods Schematic feature extractor backbone are pre-trained from training set using networks like InceptonV3, ResNet-34 LaSO network are implemented as Multi-Layer Perceptrons consisting of 3 or 4 blocks. steps: randomly select a pair of images \\(X\\) and \\(Y\\), mapping them into the feature space as \\(F_X\\) and \\(F_Y\\). \\(X\\) and \\(Y\\) are corresponding set of multiple labels, \\(L(X), L(Y) \\subseteq\\mathcal{L}\\) The LaSO network \\(M_{int}\\), \\(M_{uni}\\) and \\(M_{sub}\\) receive the concatenated \\(F_X\\) and \\(F_Y\\) and are trained to synthesize feature vectors in the same feature space. That mimic the set operations corresponding intersection, union and subtraction.for example, \\(M_{int}(F_X,F_Y)=Z_{int}\\in\\mathcal{F}\\),suppose having generateed a new image \\(I\\), so the feature vector of \\(I\\) is \\(Z_{int}\\), the multi-label of \\(I\\) is \\(L(I)=L(X)\\cap L(B)\\), same meaning as the two leaving operations. source feature vectors, \\(F_X\\), \\(F_Y\\), and the outputs of LaSO networks, namely \\(Z_{int}\\), \\(Z_{uni}\\), \\(Z_{sub}\\) are fed into a classifier \\(C\\) loss function: Need to train two networks, classifier \\(C\\) and LaSO network. Use the BCE(Binary Cross-Entropy) loss. $BCE(s,l) = -_{i}l_ilog(s_i)+(1-l_i)log(1-(s_i)) $ where vector \\(s\\) being the classifier scores, \\(l\\) being the desired(binary) labels vector, and \\(i\\) the class indices. \\(C_{loss}=BCE(C(F_X),L(X))+BCE(C(F_Y),L(Y))\\) \\(LaSO_{loss} = BCE(C(Z_{int}),L(X)\\cap L(Y))+BCE(C(Z_{uni}),L(X)\\cup L(Y)) + BCE(C(Z_{sub}), L(X)\\setminus L(Y))\\) for the LaSO updates the classifier \\(C\\) is kept fixed and only used for passing gradient backwards. add a set of Mean Square Error based restruction losses. First is used to enforce symmetry for the symmetric intersection and union operation. The second is used to reduce of mode collapse. 5.Evaluate use batch size of 16, initial learning rate of 0.001, learning rate reduced on loss plateau with factor 0.3. On all compared approaches the classifier trained on each of the episodes were trained using 40 SGD epochs. Use Adam optimizer with parameters(0.9, 0.999) evaluate on two datasets: MS-COCO, CelebA episode construction: Because each image has multiple labels, so to form a episode, we need to ensure that 1 example per category appears in the episode for 1-shot task and 5 examples category appears in the episode for 5-shot task. Due to the random nature of the episodes, this balancing is not always possible. 6.Conclusion LaSO networks can generalize to unseen categories and support their use for performing augmentation synthesis for few-shot multi-label learning 7.Notes This paper also evaluate the label set manipulation capability of the LaSO networks, to make sure this manipulation is useful. This paper also compare the approximations set operations(without training) with LaSO networks. Although it is a few-shot learning task, the datasets used to training are huge(MS-COCO, CelebA)... 8.Reference A. J. Ratner, H. R. Ehrenberg, Z. Hussain, J. Dunnmon, and C. R´e. Learning to Compose Domain-Specific Transforma- tions for Data Augmentation. (Nips), 2017.","categories":[],"tags":[{"name":"科研","slug":"科研","permalink":"http://yoursite.com/tags/%E7%A7%91%E7%A0%94/"},{"name":"论文笔记","slug":"论文笔记","permalink":"http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}]},{"title":"Matching Network for One Shot Learning","slug":"article-1","date":"2019-12-11T06:50:49.000Z","updated":"2019-12-11T11:13:14.334Z","comments":true,"path":"2019/12/11/article-1/","link":"","permalink":"http://yoursite.com/2019/12/11/article-1/","excerpt":"0. Matching Network for One Shot Learning 1. Summary This paper is a metric-based method for few-shot learning task. It uses attention mechanism and considers to embed the whole support set into feature space not just a single sample in the support set. Given a query image, the predict output is the weighted sum of the samples in the support set.","text":"0. Matching Network for One Shot Learning 1. Summary This paper is a metric-based method for few-shot learning task. It uses attention mechanism and considers to embed the whole support set into feature space not just a single sample in the support set. Given a query image, the predict output is the weighted sum of the samples in the support set. 2. Reaserch Object Learning from a few examples, one-shot learning 3. Problem Statement Train a classifer, test with a few(eg. 1-shot, 5-shot) queries(images) of unknown classes. 4. Methods attention, memory overall architecture the simplest form to compuete \\(\\hat{y}\\) of the test example \\(\\hat x\\): \\(\\hat y = \\sum\\limits_{i=1}^ka(\\hat{x},x_i)y_i\\) \\(x_i\\) and \\(y_i\\) are the samples and labels from support set, \\(a\\) is an attention mechanism the choose of attention kernel the simplest form is used in this paper: \\(a(\\hat{x},x_i)=e^{c(f(\\hat{x}),g(x_j))}/\\sum _{j=1}^ke^{c(f(\\hat{x}),g(x_j))}\\) \\(f\\) and \\(g\\) are embedding functions, which can be CNN for image classification task and word embedding for language task. \\(c\\) is the cosine distance. full context embeddings consider all the images of the support set, not just embeding one image at once. Use the bidirectional Long-Short Term Memory to encode \\(x_i\\) in the context of the support set. play the role of embedding function \\(f\\) training strategy task \\(T\\) has the distribution over label set \\(L\\), first sample a label set \\(L\\) from \\(T\\), then use \\(L\\) to sample support set \\(S\\) and a batch \\(B\\)(&quot;batch&quot; in there means queries for the given support set). training objective \\(\\theta = \\mathop{\\arg\\max}_\\theta E_{L\\sim T}\\left[E_{S\\sim L, B\\sim L}\\left[\\sum\\limits_{(x,y)\\in B}\\mathop{\\log}P_\\theta(y|x,S)\\right]\\right]\\) 5. Evaluate test and train conditions must match ran on three data sets: two image classification sets(Omniglot, ImageNet), one language model(Penn Treebank) Omniglot: Augmented the data set. Pick N unseen character classes, indepedent of alphabet, as \\(L\\), provide the model with one drawing of each of the N characters as \\(S\\sim L\\) and a batch \\(B\\sim L\\). ImageNet: Get three subdataset of the ImageNet: randImageNet(remove 118 labels at random from training set), dogImageNet(remove dog class), miniImageNet. Test on the three subdatasets. 6. Conclusion Fully Conditional Embeddings didn't seem to help much on the simple dataset, like Omniglot, but on the harder task(miniImageNet), FCE is sensible. one-shot learning is much easier if you train the network to do one-shot learning Non-parametric structures in a neural network make it easier for network to remember and adapt to new training sets in the same task. 7. Notes FCE consider the whole support set \\(S\\) instead of pair-wise comparisons. they tested their method trained on Omniglot on the MNIST dataset, acc about 72% What's the loss function of this model? Need to know more about FCE's details. ## 8. Reference O Vinyals, S Bengio, and M Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391, 2015.","categories":[],"tags":[{"name":"科研","slug":"科研","permalink":"http://yoursite.com/tags/%E7%A7%91%E7%A0%94/"},{"name":"论文笔记","slug":"论文笔记","permalink":"http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-12-11T05:22:43.115Z","updated":"2019-12-11T10:33:27.206Z","comments":true,"path":"2019/12/11/hello-world/","link":"","permalink":"http://yoursite.com/2019/12/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[]}]}